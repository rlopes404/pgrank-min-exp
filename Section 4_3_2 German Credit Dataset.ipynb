{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Dataset Experiment (German Credit Dataset-- Group Fairness)\n",
    "Generate the dataset for LTR setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, ticker, cm\n",
    "\n",
    "from pprint import pprint\n",
    "from progressbar import progressbar\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29551/1285856066.py:2: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'png')\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "plt.rcParams['text.latex.preamble'] = r\"\\usepackage{subdepth}, \\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramon\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_yahoo_dataset import on_policy_training\n",
    "from YahooDataReader import YahooDataReader\n",
    "import torch\n",
    "from models import NNModel, LinearModel\n",
    "from evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GermanCredit/german_train_rank.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29551/2993786372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYahooDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GermanCredit/german_train_rank.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYahooDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GermanCredit/german_test_rank.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GermanCredit/german_train_rank.pkl'"
     ]
    }
   ],
   "source": [
    "# read data \n",
    "import pickle as pkl\n",
    "dr = YahooDataReader(None)\n",
    "dr.data = pkl.load(open(\"GermanCredit/german_train_rank.pkl\", \"rb\"))\n",
    "vdr = YahooDataReader(None)\n",
    "vdr.data = pkl.load(open(\"GermanCredit/german_test_rank.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "args = Namespace(conditional_model=True, gpu_id=None, progressbar=True, evaluate_interval=250, input_dim=29, \n",
    "                 eval_rank_limit=1000,\n",
    "                fairness_version=\"asym_disparity\", entropy_regularizer=0.0, save_checkpoints=False, num_cores=1,\n",
    "                pooling='concat_avg', dropout=0.0, hidden_layer=8, summary_writing=False, \n",
    "                 group_fairness_version=\"asym_disparity\",early_stopping=False, lr_scheduler=False, \n",
    "                 validation_deterministic=False, evalk=1000, reward_type=\"ndcg\", baseline_type=\"value\", \n",
    "                 use_baseline=True, entreg_decay=0.0, skip_zero_relevance=True, eval_temperature=1.0, optimizer=\"Adam\",\n",
    "                clamp=False)\n",
    "torch.set_num_threads(args.num_cores)\n",
    "args.progressbar = False \n",
    "\n",
    "args.group_feat_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params_list = []\n",
    "lambdas_list = [0.0, 0.1, 1.0, 10.0, 12.0, 15.0, 20.0, 25.0, 50.0, 100.0]\n",
    "plt_data_pl = np.zeros((len(lambdas_list)+1, 2))\n",
    "for i, lgroup in enumerate(lambdas_list):\n",
    "        torch.set_num_threads(args.num_cores)\n",
    "        args.lambda_reward = 1.0\n",
    "        args.lambda_ind_fairness = 0.0\n",
    "        args.lambda_group_fairness = lgroup\n",
    "        \n",
    "        args.lr = 0.001\n",
    "        args.epochs = 10\n",
    "        args.progressbar = False\n",
    "        args.weight_decay = 0.0\n",
    "        args.sample_size = 25\n",
    "        args.optimizer = \"Adam\"\n",
    "\n",
    "        model = LinearModel(D=args.input_dim)\n",
    "\n",
    "        model = on_policy_training(dr, vdr, model, args=args)\n",
    "        if i == 0:\n",
    "            results = evaluate_model(model, vdr, fairness_evaluation=False, group_fairness_evaluation=True, \n",
    "                                 deterministic=True, args=args, num_sample_per_query=20)\n",
    "            print(results)\n",
    "            plt_data_pl[0] = [results[\"ndcg\"], results[\"avg_group_asym_disparity\"]]\n",
    "        results = evaluate_model(model, vdr, fairness_evaluation=False, group_fairness_evaluation=True, \n",
    "                                 deterministic=False, args=args, num_sample_per_query=20)\n",
    "        print(results)\n",
    "        model_params_list.append(model.w.weight.data.tolist()[0])\n",
    "        print(\"Learnt model for lambda={} has model weights as {}\".format(lgroup, model_params_list[-1]))\n",
    "        plt_data_pl[i+1] = [results[\"ndcg\"], results[\"avg_group_asym_disparity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def assign_groups(groups):\n",
    "    G = [[], []]\n",
    "    for i in range(len(groups)):\n",
    "        G[groups[i]].append(i)\n",
    "    return G\n",
    "\n",
    "\n",
    "def fair_rank(relevances, groups ,lmda = 1):\n",
    "    n = len(relevances)\n",
    "    pos_bias = vvector(n)\n",
    "    G = assign_groups(groups)\n",
    "    n_g, n_i = 0, 0\n",
    "    n_g += (len(G)-1)*len(G)\n",
    "    n_c = n**2 + n_g \n",
    "    \n",
    "    \n",
    "    c = np.ones(n_c)\n",
    "    c[:n**2] *= -1\n",
    "    c[n**2:] *= lmda\n",
    "    A_eq = []\n",
    "    #For each Row\n",
    "    for i in range(n):\n",
    "        A_temp = np.zeros(n_c)\n",
    "        A_temp[i*n:(i+1)*n] = 1\n",
    "        assert(sum(A_temp)==n)\n",
    "        A_eq.append(A_temp)\n",
    "        c[i*n:(i+1)*n] *= relevances[i]\n",
    "\n",
    "    #For each coloumn\n",
    "    for i in range(n):\n",
    "        A_temp = np.zeros(n_c)\n",
    "        A_temp[i:n**2:n] = 1\n",
    "        assert(sum(A_temp)==n)\n",
    "        A_eq.append(A_temp)\n",
    "        #Optimization \n",
    "        c[i:n**2:n] *= pos_bias[i] \n",
    "    b_eq = np.ones(n*2) \n",
    "    A_eq = np.asarray(A_eq)\n",
    "    bounds = [(0,1) for _ in range(n**2)] + [(0,None) for _ in range(n_g+n_i)]\n",
    "    \n",
    "    \n",
    "    A_ub = []\n",
    "    b_ub = np.zeros(n_g)\n",
    "    sum_rels = []\n",
    "    for group in G:\n",
    "        #Avoid devision by zero\n",
    "        sum_rel = np.max([np.sum(np.asarray(relevances)[group]), 0.01])\n",
    "        sum_rels.append(sum_rel)\n",
    "    comparisons = list(permutations(np.arange(len(G)),2))\n",
    "    j = 0\n",
    "    for a,b in comparisons:\n",
    "        f = np.zeros(n_c)\n",
    "        if len(G[a]) > 0 and len(G[b])>0 and sum_rels[a]/len(G[a]) >= sum_rels[b]/len(G[b]):\n",
    "            for i in range(n):\n",
    "                tmp1 = len(G[a]) / sum_rels[a] if i in G[a] else 0 \n",
    "                tmp2 = len(G[b]) / sum_rels[b] if i in G[b] else 0 \n",
    "                #f[i*n:(i+1)*n] *= max(0, sign*(tmp1 - tmp2))\n",
    "                f[i*n:(i+1)*n] =  (tmp1 - tmp2)\n",
    "            for i in range(n):\n",
    "                f[i:n**2:n] *= pos_bias[i]\n",
    "            f[n**2+j] = -1\n",
    "        j += 1\n",
    "        A_ub.append(f)\n",
    "\n",
    "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, \n",
    "                                  method = \"interior-point\")#, options=dict(tol=1e-12),)\n",
    "    if res.success is False:\n",
    "        print(\"Constraint not satisfied!!\")\n",
    "    probabilistic_ranking = np.reshape(res.x[:n**2],(n,n))\n",
    "    return probabilistic_ranking, res, res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from baselines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression(fit_intercept=False, normalize=False)\n",
    "feats, rel = dr.data\n",
    "feats = np.array([item for sublist in feats for item in sublist])\n",
    "rel = np.array([item for sublist in rel for item in sublist])\n",
    "model.fit(feats, rel)\n",
    "# predictions on validation\n",
    "feats, rel = vdr.data\n",
    "se_sum = 0\n",
    "length = 0\n",
    "predicted_rels = []\n",
    "for i, query in enumerate(feats):\n",
    "    rel_pred = model.predict(query)\n",
    "    predicted_rels.append(rel_pred)\n",
    "    se_sum += np.sum((rel_pred - rel[i])**2)\n",
    "    length += len(rel[i])\n",
    "print(se_sum/ length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each lambda\n",
    "# now for each query, estimate all relevances\n",
    "# we find a fair ranking, find it's loss w.r.t true relevances\n",
    "lmbdas = np.linspace(0, 0.2, 20)\n",
    "\n",
    "plt_data = np.zeros((len(lmbdas), 2))\n",
    "\n",
    "for j, lmbda in enumerate(lmbdas):\n",
    "    test_losses = []\n",
    "    test_ndcgs = []\n",
    "    \n",
    "    for i in range(len(predicted_rels)):\n",
    "        true_rels = rel[i]\n",
    "        pred_rels = predicted_rels[i]\n",
    "        groups = np.array(feats[i][:, args.group_feat_id], dtype=np.int)\n",
    "        n = len(groups)\n",
    "        \n",
    "        P, _, _ = fair_rank(pred_rels, groups, lmbda)\n",
    "        \n",
    "        test_loss = get_fairness_loss(P, true_rels, vvector(n), groups)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        test_ndcg = get_ndcg(P, true_rels, vvector(n))\n",
    "        test_ndcgs.append(test_ndcg)\n",
    "        \n",
    "    plt_data[j] = [np.mean(test_ndcgs), np.mean(test_losses)]\n",
    "        \n",
    "        \n",
    "    print(\"Lambda: {}, Average Test Fairness Loss: {}, Average Test NDCG: {}\".format(lmbda, \n",
    "                                                     np.mean(test_losses), np.mean(test_ndcgs)))\n",
    "\n",
    "plt.scatter(plt_data[:, 0], plt_data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(plt_data[:, 0], plt_data[:, 1])\n",
    "plt.scatter(plt_data_pl[:, 0], plt_data_pl[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zehlike import *\n",
    "\n",
    "args.lr = [0.001]\n",
    "args.lambda_reward = 1.0\n",
    "plt_data_4 = []\n",
    "lambdas = [0.0, 1.0, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "args.weight_decay = [0.0]\n",
    "args.epochs = [10]\n",
    "disparities_mat = np.zeros((len(lambdas), 1))\n",
    "ndcg_mat = np.zeros((len(lambdas), 1))\n",
    "for i, lg in enumerate(lambdas):\n",
    "    args.lambda_group_fairness = lg\n",
    "    model = LinearModel(D=args.input_dim)\n",
    "    model = demographic_parity_train(model, dr, vdr, vvector(200), args)\n",
    "    results = evaluate_model(\n",
    "            model,\n",
    "            vdr,\n",
    "            fairness_evaluation=False,\n",
    "            group_fairness_evaluation=True,\n",
    "            deterministic=True,\n",
    "            args=args,\n",
    "            num_sample_per_query=100)\n",
    "    plt_data_4.append([results[\"ndcg\"], results[\"avg_group_asym_disparity\"]])\n",
    "    ndcg_mat[i, 0], disparities_mat[i,0] = results[\"ndcg\"], results[\"avg_group_asym_disparity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data_4 = np.array(plt_data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(plt_data_4[:, 0], plt_data_4[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_vs_disparity_plot(plt_data_mats, names, join=False, ranges=None):\n",
    "    plt.figure(figsize=(6.5, 4))\n",
    "    if ranges:\n",
    "        plt.xlim(ranges[0])\n",
    "        plt.ylim(ranges[1])\n",
    "    initialize_params()\n",
    "    for i, plt_data_mat in enumerate(plt_data_mats):\n",
    "        if not join:\n",
    "            plt.scatter(\n",
    "                plt_data_mats[i][:, 0],\n",
    "                plt_data_mats[i][:, 1],\n",
    "                marker=\"*\",\n",
    "                label=names[i])\n",
    "        else:\n",
    "            plt.plot(\n",
    "                plt_data_mats[i][:, 0],\n",
    "                plt_data_mats[i][:, 1],\n",
    "                marker=\"*\",\n",
    "                linestyle='--',\n",
    "                label=names[i])\n",
    "    plt.legend(fontsize=14)\n",
    "#     plt.title(\"Utility-Fairness trade-off\",y=-0.30)\n",
    "    plt.xlabel(\"NDCG\")\n",
    "    plt.ylabel(r'$\\hat{\\mathcal{D}}_{\\rm group}$', fontsize=16)\n",
    "    plt.grid()\n",
    "    plt.savefig('./plots/german_tradeoff.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_vs_disparity_plot([plt_data, plt_data_pl,  plt_data_4], [\"Post-Processing ($\\lambda \\in [0, 0.2]$)\",\n",
    "                      \"Our Method ($\\lambda \\in [0,100]$ )\", \n",
    "                      \"Zehlike \\& Castillo 2018 ($\\lambda \\in [0, 10^6]$)\"], join=True, ranges=[[0.60, 0.85], [0.00, 0.054]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_vs_disparity_plot([plt_data, plt_data_pl], [\"Post-Processing ($\\lambda \\in [0, 0.2]$)\",\n",
    "                      \"Our Method ($\\lambda \\in [0,100]$ )\"], join=True, ranges=[[0.60, 0.85], [0.00, 0.054]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
